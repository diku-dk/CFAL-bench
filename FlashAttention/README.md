https://github.com/openai/triton/blob/dc9e3063d73d2410e1855e1ff258aa90a6158548/python/triton/ops/flash_attention.py

https://papers.nips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Supplemental-Conference.pdf

Suggested parameters for benchmarks:
- head dimension d in {64, 128}
- sequence length N in {512, 1024, 2048, 4096, 8192, 16384}

use Array: all;
use Benchmarking: all;
use StdIO: all;
use CommandLine: all;
use String: {strcmp, strtoi, strtof};

#define REAL float
#define tor tof

inline 
REAL maximum(REAL[.] x)
{
  return with {
    (0 * shape(x) <= iv < shape(x)): x[iv];
  }: fold(max, x[0]);
}

inline 
REAL[m, n] stabilise(REAL[m, n] x)
{
  return {[i] -> x[i] - maximum(x[i])};
}

inline
REAL[m, n] exp(REAL[m, n] X)
{
  return {iv -> Math::exp(X[iv])};
}

inline
REAL[m, n] matmul(REAL[m, k] A, REAL[k, n] B)
{
//  return {[i, j] -> sum({[p] -> A[i, p] * B[p, j]})};
  return {[i, j] -> with {
                      ([0] <= [p] < [k]): A[i, p] * B[p, j];
                    }: fold(+, tor(0))
                  | [i, j] < [m, n]};
}

inline
REAL[m, n] matmulT(REAL[m, k] A, REAL[n, k] B)
{
//  return {[i, j] -> sum({[p] -> A[i, p] * B[p, j]})};
  return {[i, j] -> with {
                      ([0] <= [p] < [k]): A[i, p] * B[j, p];
                    }: fold(+, tor(0))
                  | [i, j] < [m, n]};
}

inline
REAL[m, n] scale(REAL[m, n] x)
{
//  return {[i] -> x[i] / sum(x[i])};
  return {[i] -> x[i] / with {
                          ([0] <= [j] < [n]): x[i, j];
                        }: fold(+, tor(0))};
}

specialize REAL[512, 64, 64] FlashAttention(REAL[512, 64, 64] Q, REAL[32768, 64] K, REAL[32768, 64] V);
specialize REAL[256, 64, 64] FlashAttention(REAL[256, 64, 64] Q, REAL[16384, 64] K, REAL[16384, 64] V);
specialize REAL[64, 128, 128] FlashAttention(REAL[64, 128, 128] Q, REAL[8192, 128] K, REAL[8192, 128] V);
specialize REAL[128, 128, 128] FlashAttention(REAL[128, 128, 128] Q, REAL[16384, 128] K, REAL[16384, 128] V);
noinline
REAL[Nd, d, d] FlashAttention(REAL[Nd, d, d] Q, REAL[N, d] K, REAL[N, d] V) | (Nd == N / d)
{
//  Kt = {[i, j] -> K[j, i]};
  Vt = {[i, j] -> V[j, i]};
  return {[i] -> matmulT(scale(exp(stabilise(matmulT(Q[i], K)))), Vt)};
}

REAL L2(REAL[*] x)
{
  return Math::sqrt(sum(x * x));
}

int main()
{
  if (argc() != 3) {
    printf("%s <d> <N>\n", argv(0));
    N = 0;
    d = 0;
  } else {
    d, unused = strtoi(argv(1), 10);
    N, unused = strtoi(argv(2), 10);
    if (mod(N, d) != 0) {
      fprintf(stderr, "Please make sure d | N\n");
      d = 0;
      N = 0;
    }
  }

  fprintf(stderr, "Running with d = %d, N = %d\n", d, N);

  /* QK^t is all ds, so the softmax is e^{d} / (N e^d) = 1 / N.
     Multiplying that with V gives all 1s again.
     Taking the L2 norm of this is sqrt(d * N) */
  Q = _hideValue_SxA_(0, {[i, j, k] -> tor(1) / (tor(1) + tor(i)) | [i, j, k] < [N / d, d, d]});
  K = _hideValue_SxA_(0, {[i, j] -> tor(1) | [i, j] < [N, d]});
  V = _hideValue_SxA_(0, {[i, j] -> tor(1) | [i, j] < [N, d]});

  i_flash = getInterval("flash", 1);
  start(i_flash);

  O = FlashAttention(Q, K, V);

  end(i_flash);
  time, unit = returnResultUnit(i_flash);

  fprintf(stderr, "L2 norm of output is %lf, should be %lf\n", 
                    L2(O), Math::sqrt(tor(d * N)));
  printf("%f\n", 
        4d * tod(N) * tod(N) * (tod(d) + 1d) / time / 1e9, unit);

  return 0;
}
